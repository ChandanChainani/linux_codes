{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spin Lock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spin Lock\n",
    "\n",
    "### include/linux/spinlock_types.h\n",
    "\n",
    "```c\n",
    "typedef struct spinlock {\n",
    "\tunion {\n",
    "\t\tstruct raw_spinlock rlock;\n",
    "\n",
    "#ifdef CONFIG_DEBUG_LOCK_ALLOC\n",
    "# define LOCK_PADSIZE (offsetof(struct raw_spinlock, dep_map))\n",
    "\t\tstruct {\n",
    "\t\t\tu8 __padding[LOCK_PADSIZE];\n",
    "\t\t\tstruct lockdep_map dep_map;\n",
    "\t\t};\n",
    "#endif\n",
    "\t};\n",
    "} spinlock_t;\n",
    "\n",
    "```\n",
    "\n",
    "```c\n",
    "typedef struct raw_spinlock {\n",
    "\tarch_spinlock_t raw_lock;\n",
    "#ifdef CONFIG_GENERIC_LOCKBREAK\n",
    "\tunsigned int break_lock;\n",
    "#endif\n",
    "#ifdef CONFIG_DEBUG_SPINLOCK\n",
    "\tunsigned int magic, owner_cpu;\n",
    "\tvoid *owner;\n",
    "#endif\n",
    "#ifdef CONFIG_DEBUG_LOCK_ALLOC\n",
    "\tstruct lockdep_map dep_map;\n",
    "#endif\n",
    "} raw_spinlock_t;\n",
    "```\n",
    "\n",
    "1. `arch_spinlock_t` is a architecture-specific `spinlock`. On x86_64, it is `qspinlock`.\n",
    "\n",
    "-------------------------\n",
    "\n",
    "### some functions description\n",
    "\n",
    "`spin_lock_init` - produces initialization of the given spinlock;\n",
    "`spin_lock` - acquires given spinlock;\n",
    "`spin_lock_bh` - disables software interrupts and acquire given spinlock;\n",
    "`spin_lock_irqsave` and `spin_lock_irq` - disable interrupts on local processor, preserve/not preserve previous interrupt state in the flags and acquire given spinlock;\n",
    "`spin_unlock` - releases given spinlock;\n",
    "`spin_unlock_bh` - releases given spinlock and enables software interrupts;\n",
    "`spin_is_locked` - returns the state of the given spinlock;\n",
    "\n",
    "---------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queued Spin Lock\n",
    "\n",
    "简单的spin lock实现以及存在的问题\n",
    "\n",
    "```c\n",
    "int lock(lock)\n",
    "{\n",
    "    while (test_and_set(lock) == 1)\n",
    "        ;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "int unlock(lock)\n",
    "{\n",
    "    lock=0;\n",
    "\n",
    "    return lock;\n",
    "}\n",
    "```\n",
    "\n",
    "存在的问题\n",
    "\n",
    "1. 这种spin lock并不是先到先得的。多个线程在一个变量上spin，谁先获取具有随机性。并不是先到先得\n",
    "\n",
    "2. 由于多个CPU线程均在同一个共享变量lock上自旋，而申请和释放锁的时候必须对lock进行修改，这将导致所有参与排队自旋锁操作的处理器的缓存变得无效。如果排队自旋锁竞争比较激烈的话，频繁的缓存同步操作会导致繁重的系统总线和内存的流量，从而大大降低了系统整体的性能。\n",
    "\n",
    "-------------------------\n",
    "\n",
    "### MCS Spin Lock\n",
    "\n",
    "[MCS](http://www.cs.rochester.edu/~scott/papers/1991_TOCS_synch.pdf)\n",
    "\n",
    "The basic idea of the MCS lock is that a thread spins on a local variable and each processor in the system has its own copy of this variable (see the previous paragraph). In other words this concept is built on top of the per-cpu variables concept in the Linux kernel.\n",
    "\n",
    "When the first thread wants to acquire a lock, it registers itself in the queue. In other words it will be added to the special queue and will acquire lock, because it is free for now. When the second thread wants to acquire the same lock before the first thread release it, this thread adds its own copy of the lock variable into this queue. In this case the first thread will contain a next field which will point to the second thread. From this moment, the second thread will wait until the first thread release its lock and notify next thread about this event. The first thread will be deleted from the queue and the second thread will be owner of a lock.\n",
    "\n",
    "pesudocode:\n",
    "\n",
    "```c\n",
    "void lock() \n",
    "{\n",
    "    lock.next = NULL;\n",
    "    ancestor = put_lock_to_queue_and_return_ancestor(queue, lock); \n",
    "\n",
    "    if(ancestor) {\n",
    "        lock.is_lock = 1;\n",
    "        ancestor.next = lock;\n",
    "        while(lock.is_lock);\n",
    "    }\n",
    "}\n",
    "\n",
    "void unlock()\n",
    "{\n",
    "    if(lock.next != NULL) {\n",
    "        lock.next.is_locked = false;\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "### kernel/locking/mcs_spinlock.h\n",
    "\n",
    "```c\n",
    "struct mcs_spinlock {\n",
    "\tstruct mcs_spinlock *next;\n",
    "\tint locked; /* 1 if lock acquired */\n",
    "\tint count;  /* nesting count, see qspinlock.c */\n",
    "};\n",
    "\n",
    "```\n",
    "1. mcs_spinlock node. \n",
    "\n",
    "----------------------\n",
    "\n",
    "```c\n",
    "#ifndef arch_mcs_spin_lock_contended\n",
    "/*\n",
    " * Using smp_load_acquire() provides a memory barrier that ensures\n",
    " * subsequent operations happen after the lock is acquired.\n",
    " */\n",
    "#define arch_mcs_spin_lock_contended(l)\t\t\t\t\t\\\n",
    "do {\t\t\t\t\t\t\t\t\t\\\n",
    "\twhile (!(smp_load_acquire(l)))\t\t\t\t\t\\\n",
    "\t\tcpu_relax_lowlatency();\t\t\t\t\t\\\n",
    "} while (0)\n",
    "#endif\n",
    "\n",
    "#ifndef arch_mcs_spin_unlock_contended\n",
    "/*\n",
    " * smp_store_release() provides a memory barrier to ensure all\n",
    " * operations in the critical section has been completed before\n",
    " * unlocking.\n",
    " */\n",
    "#define arch_mcs_spin_unlock_contended(l)\t\t\t\t\\\n",
    "\tsmp_store_release((l), 1)\n",
    "#endif\n",
    "```\n",
    "\n",
    "1. acquire/release 语义。注释说的很清楚\n",
    "\n",
    "2. `cpu_relax_lowlatency` 就是些 nop 指令\n",
    "\n",
    "```c\n",
    "/* REP NOP (PAUSE) is a good thing to insert into busy-wait loops. */\n",
    "static __always_inline void rep_nop(void)\n",
    "{\n",
    "\tasm volatile(\"rep; nop\" ::: \"memory\");\n",
    "}\n",
    "\n",
    "static __always_inline void cpu_relax(void)\n",
    "{\n",
    "\trep_nop();\n",
    "}\n",
    "```\n",
    "-----------------------\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Note: the smp_load_acquire/smp_store_release pair is not\n",
    " * sufficient to form a full memory barrier across\n",
    " * cpus for many architectures (except x86) for mcs_unlock and mcs_lock.\n",
    " * For applications that need a full barrier across multiple cpus\n",
    " * with mcs_unlock and mcs_lock pair, smp_mb__after_unlock_lock() should be\n",
    " * used after mcs_lock.\n",
    " */\n",
    "\n",
    "/*\n",
    " * In order to acquire the lock, the caller should declare a local node and\n",
    " * pass a reference of the node to this function in addition to the lock.\n",
    " * If the lock has already been acquired, then this will proceed to spin\n",
    " * on this node->locked until the previous lock holder sets the node->locked\n",
    " * in mcs_spin_unlock().\n",
    " */\n",
    "static inline\n",
    "void mcs_spin_lock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n",
    "{\n",
    "\tstruct mcs_spinlock *prev;\n",
    "\n",
    "\t/* Init node */\n",
    "\tnode->locked = 0;\n",
    "\tnode->next   = NULL;\n",
    "\n",
    "\tprev = xchg_acquire(lock, node);\n",
    "\tif (likely(prev == NULL)) {\n",
    "\t\t/*\n",
    "\t\t * Lock acquired, don't need to set node->locked to 1. Threads\n",
    "\t\t * only spin on its own node->locked value for lock acquisition.\n",
    "\t\t * However, since this thread can immediately acquire the lock\n",
    "\t\t * and does not proceed to spin on its own node->locked, this\n",
    "\t\t * value won't be used. If a debug mode is needed to\n",
    "\t\t * audit lock status, then set node->locked value here.\n",
    "\t\t */\n",
    "\t\treturn;\n",
    "\t}\n",
    "\tWRITE_ONCE(prev->next, node);\n",
    "\n",
    "\t/* Wait until the lock holder passes the lock down. */\n",
    "\tarch_mcs_spin_lock_contended(&node->locked);\n",
    "}\n",
    "\n",
    "/*\n",
    " * Releases the lock. The caller should pass in the corresponding node that\n",
    " * was used to acquire the lock.\n",
    " */\n",
    "static inline\n",
    "void mcs_spin_unlock(struct mcs_spinlock **lock, struct mcs_spinlock *node)\n",
    "{\n",
    "\tstruct mcs_spinlock *next = READ_ONCE(node->next);\n",
    "\n",
    "\tif (likely(!next)) {\n",
    "\t\t/*\n",
    "\t\t * Release the lock by setting it to NULL\n",
    "\t\t */\n",
    "\t\tif (likely(cmpxchg_release(lock, node, NULL) == node))\n",
    "\t\t\treturn;\n",
    "\t\t/* Wait until the next pointer is set */\n",
    "\t\twhile (!(next = READ_ONCE(node->next)))\n",
    "\t\t\tcpu_relax_lowlatency();\n",
    "\t}\n",
    "\n",
    "\t/* Pass lock to next waiter. */\n",
    "\tarch_mcs_spin_unlock_contended(&next->locked);\n",
    "}\n",
    "\n",
    "```\n",
    "1. 很不错的队列实现\n",
    "\n",
    "2. `lock` 就是指向共同spin lock的指针，永远指向queue的队尾。`node`是本地变量\n",
    "\n",
    "3. `mcs_spin_lock` 通过 xchg 原子操作，把队尾指针设为自己，同时返回原来的指针。`WRITE_ONCE(prev->next, node);` 与前面xchg修改队尾无法做成原子操作，如果在他们中间，prev 去unlock了，这时候`prev->next` 还是 NULL，所以 prev unlock 的时候一定要等到 `prev->next != NULL` 了，再去unlock。这就是为啥unlock里面会有while等待\n",
    "\n",
    "4. release/acquire 语义在这里其实满足不了，comment里面已经说了，但实际 xchg_acauire/cmpxchg_release 都最终替换成了full memory barrier\n",
    "\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "### include/asm-generic/qspinlock_types.h\n",
    "\n",
    "```c\n",
    "typedef struct qspinlock {\n",
    "\tatomic_t\tval;\n",
    "} arch_spinlock_t;\n",
    "\n",
    "/*\n",
    " * Bitfields in the atomic value:\n",
    " *\n",
    " * When NR_CPUS < 16K\n",
    " *  0- 7: locked byte\n",
    " *     8: pending\n",
    " *  9-15: not used\n",
    " * 16-17: tail index\n",
    " * 18-31: tail cpu (+1)\n",
    " *\n",
    " * When NR_CPUS >= 16K\n",
    " *  0- 7: locked byte\n",
    " *     8: pending\n",
    " *  9-10: tail index\n",
    " * 11-31: tail cpu (+1)\n",
    " */\n",
    "```\n",
    "\n",
    "--------------------\n",
    "\n",
    "### kernel/locking/qspinlock.c\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The basic principle of a queue-based spinlock can best be understood\n",
    " * by studying a classic queue-based spinlock implementation called the\n",
    " * MCS lock. The paper below provides a good description for this kind\n",
    " * of lock.\n",
    " *\n",
    " * http://www.cise.ufl.edu/tr/DOC/REP-1992-71.pdf\n",
    " *\n",
    " * This queued spinlock implementation is based on the MCS lock, however to make\n",
    " * it fit the 4 bytes we assume spinlock_t to be, and preserve its existing\n",
    " * API, we must modify it somehow.\n",
    " *\n",
    " * In particular; where the traditional MCS lock consists of a tail pointer\n",
    " * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to\n",
    " * unlock the next pending (next->locked), we compress both these: {tail,\n",
    " * next->locked} into a single u32 value.\n",
    " *\n",
    " * Since a spinlock disables recursion of its own context and there is a limit\n",
    " * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there\n",
    " * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now\n",
    " * we can encode the tail by combining the 2-bit nesting level with the cpu\n",
    " * number. With one byte for the lock value and 3 bytes for the tail, only a\n",
    " * 32-bit word is now needed. Even though we only need 1 bit for the lock,\n",
    " * we extend it to a full byte to achieve better performance for architectures\n",
    " * that support atomic byte write.\n",
    " *\n",
    " * We also change the first spinner to spin on the lock bit instead of its\n",
    " * node; whereby avoiding the need to carry a node from lock to unlock, and\n",
    " * preserving existing lock API. This also makes the unlock code simpler and\n",
    " * faster.\n",
    " *\n",
    " * N.B. The current implementation only supports architectures that allow\n",
    " *      atomic operations on smaller 8-bit and 16-bit data types.\n",
    " *\n",
    " */\n",
    "```\n",
    "\n",
    "1. 内核的qspinlock 跟 mcs lock的思想是一样的，只不过把所有东西压缩到了32bit里面\n",
    "\n",
    "\n",
    "\n",
    "```c\n",
    "/*\n",
    " * By using the whole 2nd least significant byte for the pending bit, we\n",
    " * can allow better optimization of the lock acquisition for the pending\n",
    " * bit holder.\n",
    " *\n",
    " * This internal structure is also used by the set_locked function which\n",
    " * is not restricted to _Q_PENDING_BITS == 8.\n",
    " */\n",
    "struct __qspinlock {\n",
    "\tunion {\n",
    "\t\tatomic_t val;\n",
    "#ifdef __LITTLE_ENDIAN\n",
    "\t\tstruct {\n",
    "\t\t\tu8\tlocked;\n",
    "\t\t\tu8\tpending;\n",
    "\t\t};\n",
    "\t\tstruct {\n",
    "\t\t\tu16\tlocked_pending;\n",
    "\t\t\tu16\ttail;\n",
    "\t\t};\n",
    "#else\n",
    "\t\tstruct {\n",
    "\t\t\tu16\ttail;\n",
    "\t\t\tu16\tlocked_pending;\n",
    "\t\t};\n",
    "\t\tstruct {\n",
    "\t\t\tu8\treserved[2];\n",
    "\t\t\tu8\tpending;\n",
    "\t\t\tu8\tlocked;\n",
    "\t\t};\n",
    "#endif\n",
    "\t};\n",
    "};\n",
    "\n",
    "```\n",
    "\n",
    "1. 用 `union`方便的把val进行分段，nice trick\n",
    "\n",
    "-------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d47d1382e92bbaf84c50276baa079056532326c20bdaac4b09430c41eda0c22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
