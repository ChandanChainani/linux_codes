{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memory zone\n",
    "\n",
    "![zone01](resources/zone01.png)[ref](https://stackoverflow.com/questions/18061218/how-linux-kernel-decide-to-which-memory-zone-to-use)\n",
    "\n",
    "![zone02](resources/zone02.png)\n",
    "![zone03](resources/zone03.png)\n",
    "\n",
    "[x64 mem layout](https://unix.stackexchange.com/questions/509607/how-a-64-bit-process-virtual-address-space-is-divided-in-linux)\n",
    "\n",
    "[canonical address](https://en.wikipedia.org/wiki/X86-64#Virtual_address_space_details)\n",
    "\n",
    "---------------------\n",
    "\n",
    "## linux2.6/include/linux/mmzone.h\n",
    "\n",
    "```c\n",
    "#define ZONE_DMA\t\t0\n",
    "#define ZONE_NORMAL\t\t1\n",
    "#define ZONE_HIGHMEM\t\t2\n",
    "#define MAX_NR_ZONES\t\t3\n",
    "#define GFP_ZONEMASK\t0x03\n",
    "\n",
    "/*\n",
    " * One allocation request operates on a zonelist. A zonelist\n",
    " * is a list of zones, the first one is the 'goal' of the\n",
    " * allocation, the other zones are fallback zones, in decreasing\n",
    " * priority.\n",
    " *\n",
    " * Right now a zonelist takes up less than a cacheline. We never\n",
    " * modify it apart from boot-up, and only a few indices are used,\n",
    " * so despite the zonelist table being relatively big, the cache\n",
    " * footprint of this construct is very small.\n",
    " */\n",
    "struct zonelist {\n",
    "\tstruct zone *zones[MAX_NUMNODES * MAX_NR_ZONES + 1]; // NULL delimited\n",
    "};\n",
    "\n",
    "```\n",
    "1. define some consts\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The pg_data_t structure is used in machines with CONFIG_DISCONTIGMEM\n",
    " * (mostly NUMA machines?) to denote a higher-level memory zone than the\n",
    " * zone denotes.\n",
    " *\n",
    " * On NUMA machines, each NUMA node would have a pg_data_t to describe\n",
    " * it's memory layout.\n",
    " *\n",
    " * Memory statistics and page replacement data structures are maintained on a\n",
    " * per-zone basis.\n",
    " */\n",
    "struct bootmem_data;\n",
    "typedef struct pglist_data {\n",
    "\tstruct zone node_zones[MAX_NR_ZONES];\n",
    "\tstruct zonelist node_zonelists[MAX_NR_ZONES];\n",
    "\tint nr_zones;\n",
    "\tstruct page *node_mem_map;\n",
    "\tunsigned long *valid_addr_bitmap;\n",
    "\tstruct bootmem_data *bdata;\n",
    "\tunsigned long node_start_pfn;\n",
    "\tunsigned long node_present_pages; /* total number of physical pages */\n",
    "\tunsigned long node_spanned_pages; /* total size of physical page\n",
    "\t\t\t\t\t     range, including holes */\n",
    "\tint node_id;\n",
    "\tstruct pglist_data *pgdat_next;\n",
    "\twait_queue_head_t       kswapd_wait;\n",
    "} pg_data_t;\n",
    "\n",
    "#define node_present_pages(nid)\t(NODE_DATA(nid)->node_present_pages)\n",
    "#define node_spanned_pages(nid)\t(NODE_DATA(nid)->node_spanned_pages)\n",
    "\n",
    "```\n",
    "\n",
    "1. NUMA node\n",
    "\n",
    "2. `node_mem_map` array of page descriptors of the node\n",
    "\n",
    "----------------\n",
    "\n",
    "![memzone01](resources/memoryzone01.png)\n",
    "![memzone02](resources/memoryzone02.png)\n",
    "\n",
    "\n",
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * The zone field is never updated after free_area_init_core()\n",
    " * sets it, so none of the operations on it need to be atomic.\n",
    " * We'll have up to (MAX_NUMNODES * MAX_NR_ZONES) zones total,\n",
    " * so we use (MAX_NODES_SHIFT + MAX_ZONES_SHIFT) here to get enough bits.\n",
    " */\n",
    "#define NODEZONE_SHIFT (sizeof(page_flags_t)*8 - MAX_NODES_SHIFT - MAX_ZONES_SHIFT)\n",
    "#define NODEZONE(node, zone)\t((node << ZONES_SHIFT) | zone)\n",
    "\n",
    "static inline unsigned long page_zonenum(struct page *page)\n",
    "{\n",
    "\treturn (page->flags >> NODEZONE_SHIFT) & (~(~0UL << ZONES_SHIFT));\n",
    "}\n",
    "static inline unsigned long page_to_nid(struct page *page)\n",
    "{\n",
    "\treturn (page->flags >> (NODEZONE_SHIFT + ZONES_SHIFT));\n",
    "}\n",
    "\n",
    "struct zone;\n",
    "extern struct zone *zone_table[];\n",
    "\n",
    "static inline struct zone *page_zone(struct page *page)\n",
    "{\n",
    "\treturn zone_table[page->flags >> NODEZONE_SHIFT];\n",
    "}\n",
    "\n",
    "static inline void set_page_zone(struct page *page, unsigned long nodezone_num)\n",
    "{\n",
    "\tpage->flags &= ~(~0UL << NODEZONE_SHIFT);\n",
    "\tpage->flags |= nodezone_num << NODEZONE_SHIFT;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `zone_table` is a global zone array. the `node num` and `zone num` is stored in the high bits of `page->flags`\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/include/linux/gfp.h\n",
    "\n",
    "![gfp01](resources/gfp01.png)\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zone allocator\n",
    "\n",
    "![memoryzone03](resources/memoryzone03.png)\n",
    "![memoryzone04](resources/memoryzone04.png)\n",
    "\n",
    "\n",
    "1. linux 最底层都是通过buddy system来管理物理页\n",
    "\n",
    "2. 每个进程都有自己的vma（virtual memory area） list/red-black tree来管理各自的虚拟内存段\n",
    "\n",
    "3. 每个进程通过自己的多层 page table 将虚拟地址和物理地址联系起来\n",
    "\n",
    "4. 无论系统进程还是用户进程，当在实际分配物理内存页的时候，底层都是通过buddy system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## buddy system algorithm\n",
    "\n",
    "![buddy01](resources/buddy01.png)\n",
    "![buddy02](resources/buddy02.png)\n",
    "\n",
    "-----------------\n",
    "\n",
    "## linux2.6/include/linux/mmzone.h\n",
    "\n",
    "```c\n",
    "struct free_area {\n",
    "\tstruct list_head\tfree_list;\n",
    "\tunsigned long\t\t*map;\n",
    "};\n",
    "\n",
    "/*\n",
    "    * free areas of different sizes\n",
    "    */\n",
    "struct free_area\tfree_area[MAX_ORDER];\n",
    "```\n",
    "1. free_area is the buddy list array of different orders.\n",
    "\n",
    "\n",
    "## linux2.6/mm/page_alloc.c\n",
    "\n",
    "### allocate one page\n",
    "\n",
    "```c\n",
    "/* \n",
    " * Do the hard work of removing an element from the buddy allocator.\n",
    " * Call me with the zone->lock already held.\n",
    " */\n",
    "static struct page *__rmqueue(struct zone *zone, unsigned int order)\n",
    "{\n",
    "\tstruct free_area * area;\n",
    "\tunsigned int current_order;\n",
    "\tstruct page *page;\n",
    "\tunsigned int index;\n",
    "\n",
    "\tfor (current_order = order; current_order < MAX_ORDER; ++current_order) {\n",
    "\t\tarea = zone->free_area + current_order;\n",
    "\t\tif (list_empty(&area->free_list))\n",
    "\t\t\tcontinue;\n",
    "\n",
    "\t\tpage = list_entry(area->free_list.next, struct page, list);\n",
    "\t\tlist_del(&page->list);\n",
    "\t\tindex = page - zone->zone_mem_map;\n",
    "\t\tif (current_order != MAX_ORDER-1)\n",
    "\t\t\tMARK_USED(index, current_order, area);\n",
    "\t\tzone->free_pages -= 1UL << order;\n",
    "\t\treturn expand(zone, page, index, order, current_order, area);\n",
    "\t}\n",
    "\n",
    "\treturn NULL;\n",
    "}\n",
    "\n",
    "static inline struct page *\n",
    "expand(struct zone *zone, struct page *page,\n",
    "\t unsigned long index, int low, int high, struct free_area *area)\n",
    "{\n",
    "\tunsigned long size = 1 << high;\n",
    "\n",
    "\twhile (high > low) {\n",
    "\t\tBUG_ON(bad_range(zone, page));\n",
    "\t\tarea--;\n",
    "\t\thigh--;\n",
    "\t\tsize >>= 1;\n",
    "\t\tlist_add(&page->list, &area->free_list);\n",
    "\t\tMARK_USED(index, high, area);\n",
    "\t\tindex += size;\n",
    "\t\tpage += size;\n",
    "\t}\n",
    "\treturn page;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. `__rmqueue` 从 `free_area` 里面取出大小合适的block\n",
    "\n",
    "2. `expand` 裁剪block。把block最后一段>=order 的内存返回，其他的放入到对应的list里面\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### free one page\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Freeing function for a buddy system allocator.\n",
    " *\n",
    " * The concept of a buddy system is to maintain direct-mapped table\n",
    " * (containing bit values) for memory blocks of various \"orders\".\n",
    " * The bottom level table contains the map for the smallest allocatable\n",
    " * units of memory (here, pages), and each level above it describes\n",
    " * pairs of units from the levels below, hence, \"buddies\".\n",
    " * At a high level, all that happens here is marking the table entry\n",
    " * at the bottom level available, and propagating the changes upward\n",
    " * as necessary, plus some accounting needed to play nicely with other\n",
    " * parts of the VM system.\n",
    " * At each level, we keep a list of pages, which are heads of continuous\n",
    " * free pages of length of (1 << order) and marked with PG_Private.Page's\n",
    " * order is recorded in page->private field.\n",
    " * So when we are allocating or freeing one, we can derive the state of the\n",
    " * other.  That is, if we allocate a small block, and both were   \n",
    " * free, the remainder of the region must be split into blocks.   \n",
    " * If a block is freed, and its buddy is also free, then this\n",
    " * triggers coalescing into a block of larger size.            \n",
    " *\n",
    " * -- wli\n",
    " */\n",
    "\n",
    "static inline void __free_pages_bulk (struct page *page, struct page *base,\n",
    "\t\tstruct zone *zone, unsigned int order)\n",
    "{\n",
    "\tunsigned long page_idx;\n",
    "\tstruct page *coalesced;\n",
    "\tint order_size = 1 << order;\n",
    "\n",
    "\tif (unlikely(order))\n",
    "\t\tdestroy_compound_page(page, order);\n",
    "\n",
    "\tpage_idx = page - base;\n",
    "\n",
    "\tBUG_ON(page_idx & (order_size - 1));\n",
    "\tBUG_ON(bad_range(zone, page));\n",
    "\n",
    "\tzone->free_pages += order_size;\n",
    "\twhile (order < MAX_ORDER-1) {\n",
    "\t\tstruct free_area *area;\n",
    "\t\tstruct page *buddy;\n",
    "\t\tint buddy_idx;\n",
    "\n",
    "\t\tbuddy_idx = (page_idx ^ (1 << order));\n",
    "\t\tbuddy = base + buddy_idx;\n",
    "\t\tif (bad_range(zone, buddy))\n",
    "\t\t\tbreak;\n",
    "\t\tif (!page_is_buddy(buddy, order))\n",
    "\t\t\tbreak;\n",
    "\t\t/* Move the buddy up one level. */\n",
    "\t\tlist_del(&buddy->lru);\n",
    "\t\tarea = zone->free_area + order;\n",
    "\t\tarea->nr_free--;\n",
    "\t\trmv_page_order(buddy);\n",
    "\t\tpage_idx &= buddy_idx;\n",
    "\t\torder++;\n",
    "\t}\n",
    "\tcoalesced = base + page_idx;\n",
    "\tset_page_order(coalesced, order);\n",
    "\tlist_add(&coalesced->lru, &zone->free_area[order].free_list);\n",
    "\tzone->free_area[order].nr_free++;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. 每一个order block的起始地址一定是（1<<order)的整数倍，也就是(1<<(order-1))位置一定是0\n",
    "\n",
    "2. 因此，如果这个block和它同order的相邻block可以合并成（order+1）的block，那么如果这个block在（1<<order）处为0，那么就跟它后面一个同order的block合并，否则就跟它前面一个同order的block合并。因此这里寻找其buddy就直接用XOR(1<<order)。也就是减去（或者加上）（1<<order）。。。NICE！！！\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-CPU Page Frame Cache\n",
    "\n",
    "![percpu01](resources/percpu01.png)\n",
    "![percpu02](resources/percpu02.png)\n",
    "\n",
    "```c\n",
    "struct per_cpu_pages {\n",
    "\tint count;\t\t/* number of pages in the list */\n",
    "\tint low;\t\t/* low watermark, refill needed */\n",
    "\tint high;\t\t/* high watermark, emptying needed */\n",
    "\tint batch;\t\t/* chunk size for buddy add/remove */\n",
    "\tstruct list_head list;\t/* the list of pages */\n",
    "};\n",
    "\n",
    "struct per_cpu_pageset {\n",
    "\tstruct per_cpu_pages pcp[2];\t/* 0: hot.  1: cold */\n",
    "#ifdef CONFIG_NUMA\n",
    "\tunsigned long numa_hit;\t\t/* allocated in intended node */\n",
    "\tunsigned long numa_miss;\t/* allocated in non intended node */\n",
    "\tunsigned long numa_foreign;\t/* was intended here, hit elsewhere */\n",
    "\tunsigned long interleave_hit; \t/* interleaver prefered this zone */\n",
    "\tunsigned long local_node;\t/* allocation from local node */\n",
    "\tunsigned long other_node;\t/* allocation from other node */\n",
    "#endif\n",
    "} ____cacheline_aligned_in_smp;\n",
    "``` \n",
    "\n",
    "1. ZONE info can be get from /proc/zoneinfo\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linux2.6/mm/page_alloc.c\n",
    "\n",
    "### Allocating page frames though the per-CPU page frame caches\n",
    "\n",
    "\n",
    "```c\n",
    "/* \n",
    " * Obtain a specified number of elements from the buddy allocator, all under\n",
    " * a single hold of the lock, for efficiency.  Add them to the supplied list.\n",
    " * Returns the number of new pages which were placed at *list.\n",
    " */\n",
    "static int rmqueue_bulk(struct zone *zone, unsigned int order, \n",
    "\t\t\tunsigned long count, struct list_head *list)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\tint i;\n",
    "\tint allocated = 0;\n",
    "\tstruct page *page;\n",
    "\t\n",
    "\tspin_lock_irqsave(&zone->lock, flags);\n",
    "\tfor (i = 0; i < count; ++i) {\n",
    "\t\tpage = __rmqueue(zone, order);\n",
    "\t\tif (page == NULL)\n",
    "\t\t\tbreak;\n",
    "\t\tallocated++;\n",
    "\t\tlist_add_tail(&page->lru, list);\n",
    "\t}\n",
    "\tspin_unlock_irqrestore(&zone->lock, flags);\n",
    "\treturn allocated;\n",
    "}\n",
    "\n",
    "\n",
    "/*\n",
    " * Really, prep_compound_page() should be called from __rmqueue_bulk().  But\n",
    " * we cheat by calling it from here, in the order > 0 path.  Saves a branch\n",
    " * or two.\n",
    " */\n",
    "static struct page *\n",
    "buffered_rmqueue(struct zone *zone, int order, int gfp_flags)\n",
    "{\n",
    "\tunsigned long flags;\n",
    "\tstruct page *page = NULL;\n",
    "\tint cold = !!(gfp_flags & __GFP_COLD);\n",
    "\n",
    "\tif (order == 0) {\n",
    "\t\tstruct per_cpu_pages *pcp;\n",
    "\n",
    "\t\tpcp = &zone->pageset[get_cpu()].pcp[cold];\n",
    "\t\tlocal_irq_save(flags);\n",
    "\t\tif (pcp->count <= pcp->low)\n",
    "\t\t\tpcp->count += rmqueue_bulk(zone, 0,\n",
    "\t\t\t\t\t\tpcp->batch, &pcp->list);\n",
    "\t\tif (pcp->count) {\n",
    "\t\t\tpage = list_entry(pcp->list.next, struct page, lru);\n",
    "\t\t\tlist_del(&page->lru);\n",
    "\t\t\tpcp->count--;\n",
    "\t\t}\n",
    "\t\tlocal_irq_restore(flags);\n",
    "\t\tput_cpu();\n",
    "\t}\n",
    "\n",
    "\tif (page == NULL) {\n",
    "\t\tspin_lock_irqsave(&zone->lock, flags);\n",
    "\t\tpage = __rmqueue(zone, order);\n",
    "\t\tspin_unlock_irqrestore(&zone->lock, flags);\n",
    "\t}\n",
    "\n",
    "\tif (page != NULL) {\n",
    "\t\tBUG_ON(bad_range(zone, page));\n",
    "\t\tmod_page_state_zone(zone, pgalloc, 1 << order);\n",
    "\t\tprep_new_page(page, order);\n",
    "\n",
    "\t\tif (gfp_flags & __GFP_ZERO)\n",
    "\t\t\tprep_zero_page(page, order, gfp_flags);\n",
    "\n",
    "\t\tif (order && (gfp_flags & __GFP_COMP))\n",
    "\t\t\tprep_compound_page(page, order);\n",
    "\t}\n",
    "\treturn page;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `int cold = !!(gfp_flags & __GFP_COLD);` small trick to get one bit value with out >>\n",
    "\n",
    "2. ![percpu03](resources/percpu03.png)\n",
    "\n",
    "3. ![percpu04](resources/percpu04.png)\n",
    "\n",
    "------------------\n",
    "\n",
    "### Release page frames to the per-CPU page frame caches\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Free a 0-order page\n",
    " */\n",
    "static void FASTCALL(free_hot_cold_page(struct page *page, int cold));\n",
    "static void fastcall free_hot_cold_page(struct page *page, int cold)\n",
    "{\n",
    "\tstruct zone *zone = page_zone(page);\n",
    "\tstruct per_cpu_pages *pcp;\n",
    "\tunsigned long flags;\n",
    "\n",
    "\tarch_free_page(page, 0);\n",
    "\n",
    "\tkernel_map_pages(page, 1, 0);\n",
    "\tinc_page_state(pgfree);\n",
    "\tif (PageAnon(page))\n",
    "\t\tpage->mapping = NULL;\n",
    "\tfree_pages_check(__FUNCTION__, page);\n",
    "\tpcp = &zone->pageset[get_cpu()].pcp[cold];\n",
    "\tlocal_irq_save(flags);\n",
    "\tif (pcp->count >= pcp->high)\n",
    "\t\tpcp->count -= free_pages_bulk(zone, pcp->batch, &pcp->list, 0);\n",
    "\tlist_add(&page->lru, &pcp->list);\n",
    "\tpcp->count++;\n",
    "\tlocal_irq_restore(flags);\n",
    "\tput_cpu();\n",
    "}\n",
    "```\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Slab Allocator\n",
    "\n",
    "![slab01](resources/slab01.png)\n",
    "![slab02](resources/slab02.png)\n",
    "\n",
    "----------------------\n",
    "\n",
    "### cache mapping\n",
    "\n",
    "![cachemapping01](resources/cachemapping01.png)\n",
    "\n",
    "![cachemapping02](resources/cachemapping02.png)\n",
    "\n",
    "![cachemapping03](resources/cachemapping03.png)\n",
    "\n",
    "![cachemapping04](resources/cachemapping04.png)\n",
    "\n",
    "![cachemapping05](resources/cachemapping05.png)\n",
    "\n",
    "-----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slab coloring\n",
    "\n",
    "![slabcolor](resources/slabcolor01.png)\n",
    "![slabcolor](resources/slabcolor02.png)\n",
    "\n",
    "----------------------\n",
    "\n",
    "1. 我们拿到的每个slab，都会是在page对齐的位置，但是并不一定是连续的。\n",
    "\n",
    "2. 对于一个512K，8路，16Byte 长度cacheline的cache来讲，一共是512K / （8*16）= 4096 sets。如果我们分配的slab长度比较小，比如就是4K，一共有 4K / 16B = 256 个cacheline。因此也就只会用到前 256/4096 个sets。导致后面大量的cache用不到，所以引入slab coloring是有用处的\n",
    "\n",
    "3. 当slab很大的时候，这时候再加offset就没什么用了\n",
    "\n",
    "[reference](https://stackoverflow.com/questions/46731933/linux-slab-allocator-and-cache-performance/57345687#57345687)\n",
    "\n",
    "![slabcolor](resources/slabcolor03.png)\n",
    "![slabcolor](resources/slabcolor04.png)\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noncontigious Memory Area Management\n",
    "\n",
    "1. 无论是buddy system 还是 slab 分配，管理的都是physical memory pages。保证的是物理内存的连续性\n",
    "\n",
    "2. 对于大部分的应用，并不需要物理内存的连续性，只要线性内存连续就好了\n",
    "\n",
    "![nonc01](resources/noncontigious01.png)\n",
    "![nonc02](resources/noncontigious02.png)\n",
    "\n",
    "\n",
    "-------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/include/linux/vmalloc.h\n",
    "\n",
    "```c\n",
    "/* bits in vm_struct->flags */\n",
    "#define VM_IOREMAP\t0x00000001\t/* ioremap() and friends */\n",
    "#define VM_ALLOC\t0x00000002\t/* vmalloc() */\n",
    "#define VM_MAP\t\t0x00000004\t/* vmap()ed pages */\n",
    "/* bits [20..32] reserved for arch specific ioremap internals */\n",
    "\n",
    "struct vm_struct {\n",
    "\tvoid\t\t\t*addr;\n",
    "\tunsigned long\t\tsize;\n",
    "\tunsigned long\t\tflags;\n",
    "\tstruct page\t\t**pages;\n",
    "\tunsigned int\t\tnr_pages;\n",
    "\tunsigned long\t\tphys_addr;\n",
    "\tstruct vm_struct\t*next;\n",
    "};\n",
    "```\n",
    "\n",
    "1. `vm_struct` 用了记录一段虚拟地址连续的内存所对应的物理pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linux2.6/mm/vmalloc.c\n",
    "\n",
    "```c\n",
    "/**\n",
    " *\t__vmalloc  -  allocate virtually contiguous memory\n",
    " *\n",
    " *\t@size:\t\tallocation size\n",
    " *\t@gfp_mask:\tflags for the page level allocator\n",
    " *\t@prot:\t\tprotection mask for the allocated pages\n",
    " *\n",
    " *\tAllocate enough pages to cover @size from the page level\n",
    " *\tallocator with @gfp_mask flags.  Map them into contiguous\n",
    " *\tkernel virtual space, using a pagetable protection of @prot.\n",
    " */\n",
    "void *__vmalloc(unsigned long size, int gfp_mask, pgprot_t prot)\n",
    "{\n",
    "\tstruct vm_struct *area;\n",
    "\tstruct page **pages;\n",
    "\tunsigned int nr_pages, array_size, i;\n",
    "\n",
    "\tsize = PAGE_ALIGN(size);\n",
    "\tif (!size || (size >> PAGE_SHIFT) > num_physpages)\n",
    "\t\treturn NULL;\n",
    "\n",
    "\tarea = get_vm_area(size, VM_ALLOC);\n",
    "\tif (!area)\n",
    "\t\treturn NULL;\n",
    "\n",
    "\tnr_pages = size >> PAGE_SHIFT;\n",
    "\tarray_size = (nr_pages * sizeof(struct page *));\n",
    "\n",
    "\tarea->nr_pages = nr_pages;\n",
    "\t/* Please note that the recursion is strictly bounded. */\n",
    "\tif (array_size > PAGE_SIZE)\n",
    "\t\tpages = __vmalloc(array_size, gfp_mask, PAGE_KERNEL);\n",
    "\telse\n",
    "\t\tpages = kmalloc(array_size, (gfp_mask & ~__GFP_HIGHMEM));\n",
    "\tarea->pages = pages;\n",
    "\tif (!area->pages) {\n",
    "\t\tremove_vm_area(area->addr);\n",
    "\t\tkfree(area);\n",
    "\t\treturn NULL;\n",
    "\t}\n",
    "\tmemset(area->pages, 0, array_size);\n",
    "\n",
    "\tfor (i = 0; i < area->nr_pages; i++) {\n",
    "\t\tarea->pages[i] = alloc_page(gfp_mask);\n",
    "\t\tif (unlikely(!area->pages[i])) {\n",
    "\t\t\t/* Successfully allocated i pages, free them in __vunmap() */\n",
    "\t\t\tarea->nr_pages = i;\n",
    "\t\t\tgoto fail;\n",
    "\t\t}\n",
    "\t}\n",
    "\t\n",
    "\tif (map_vm_area(area, prot, &pages))\n",
    "\t\tgoto fail;\n",
    "\treturn area->addr;\n",
    "\n",
    "fail:\n",
    "\tvfree(area->addr);\n",
    "\treturn NULL;\n",
    "}\n",
    "```\n",
    "\n",
    "1. 分配一段size大小的虚拟地址连续的内存空间，并申请每一个page。\n",
    "\n",
    "2. 在申请page数组的时候，如果大小小于PAGE_SIZE，用kmalloc分配，否则调用__vmalloc\n",
    "\n",
    "\n",
    "```c\n",
    "struct vm_struct *__get_vm_area(unsigned long size, unsigned long flags,\n",
    "\t\t\t\tunsigned long start, unsigned long end)\n",
    "{\n",
    "\tstruct vm_struct **p, *tmp, *area;\n",
    "\tunsigned long align = 1;\n",
    "\tunsigned long addr;\n",
    "\n",
    "\tif (flags & VM_IOREMAP) {\n",
    "\t\tint bit = fls(size);\n",
    "\n",
    "\t\tif (bit > IOREMAP_MAX_ORDER)\n",
    "\t\t\tbit = IOREMAP_MAX_ORDER;\n",
    "\t\telse if (bit < PAGE_SHIFT)\n",
    "\t\t\tbit = PAGE_SHIFT;\n",
    "\n",
    "\t\talign = 1ul << bit;\n",
    "\t}\n",
    "\taddr = ALIGN(start, align);\n",
    "\n",
    "\tarea = kmalloc(sizeof(*area), GFP_KERNEL);\n",
    "\tif (unlikely(!area))\n",
    "\t\treturn NULL;\n",
    "\n",
    "\t/*\n",
    "\t * We always allocate a guard page.\n",
    "\t */\n",
    "\tsize += PAGE_SIZE;\n",
    "\tif (unlikely(!size)) {\n",
    "\t\tkfree (area);\n",
    "\t\treturn NULL;\n",
    "\t}\n",
    "\n",
    "\twrite_lock(&vmlist_lock);\n",
    "\tfor (p = &vmlist; (tmp = *p) != NULL ;p = &tmp->next) {\n",
    "\t\tif ((unsigned long)tmp->addr < addr) {\n",
    "\t\t\tif((unsigned long)tmp->addr + tmp->size >= addr)\n",
    "\t\t\t\taddr = ALIGN(tmp->size + \n",
    "\t\t\t\t\t     (unsigned long)tmp->addr, align);\n",
    "\t\t\tcontinue;\n",
    "\t\t}\n",
    "\t\tif ((size + addr) < addr)\n",
    "\t\t\tgoto out;\n",
    "\t\tif (size + addr <= (unsigned long)tmp->addr)\n",
    "\t\t\tgoto found;\n",
    "\t\taddr = ALIGN(tmp->size + (unsigned long)tmp->addr, align);\n",
    "\t\tif (addr > end - size)\n",
    "\t\t\tgoto out;\n",
    "\t}\n",
    "\n",
    "found:\n",
    "\tarea->next = *p;\n",
    "\t*p = area;\n",
    "\n",
    "\tarea->flags = flags;\n",
    "\tarea->addr = (void *)addr;\n",
    "\tarea->size = size;\n",
    "\tarea->pages = NULL;\n",
    "\tarea->nr_pages = 0;\n",
    "\tarea->phys_addr = 0;\n",
    "\twrite_unlock(&vmlist_lock);\n",
    "\n",
    "\treturn area;\n",
    "\n",
    "out:\n",
    "\twrite_unlock(&vmlist_lock);\n",
    "\tkfree(area);\n",
    "\tif (printk_ratelimit())\n",
    "\t\tprintk(KERN_WARNING \"allocation failed: out of vmalloc space - use vmalloc=<size> to increase size.\\n\");\n",
    "\treturn NULL;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1. `vm_list` is in `linux2.6/mm/vmalloc.c` and is a global variable to record all allocated noncontigious memory. this array is sorted by the address.\n",
    "\n",
    "```c\n",
    "DEFINE_RWLOCK(vmlist_lock);\n",
    "struct vm_struct *vmlist;\n",
    "```\n",
    "\n",
    "2. use `map_vm_area` to connect the physical pages to the page table index.\n",
    "\n",
    "```c\n",
    "int map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages)\n",
    "{\n",
    "\tunsigned long address = (unsigned long) area->addr;\n",
    "\tunsigned long end = address + (area->size-PAGE_SIZE);\n",
    "\tunsigned long next;\n",
    "\tpgd_t *pgd;\n",
    "\tint err = 0;\n",
    "\tint i;\n",
    "\n",
    "\tpgd = pgd_offset_k(address);\n",
    "\tspin_lock(&init_mm.page_table_lock);\n",
    "\tfor (i = pgd_index(address); i <= pgd_index(end-1); i++) {\n",
    "\t\tpud_t *pud = pud_alloc(&init_mm, pgd, address);\n",
    "\t\tif (!pud) {\n",
    "\t\t\terr = -ENOMEM;\n",
    "\t\t\tbreak;\n",
    "\t\t}\n",
    "\t\tnext = (address + PGDIR_SIZE) & PGDIR_MASK;\n",
    "\t\tif (next < address || next > end)\n",
    "\t\t\tnext = end;\n",
    "\t\tif (map_area_pud(pud, address, next, prot, pages)) {\n",
    "\t\t\terr = -ENOMEM;\n",
    "\t\t\tbreak;\n",
    "\t\t}\n",
    "\n",
    "\t\taddress = next;\n",
    "\t\tpgd++;\n",
    "\t}\n",
    "\n",
    "\tspin_unlock(&init_mm.page_table_lock);\n",
    "\tflush_cache_vmap((unsigned long) area->addr, end);\n",
    "\treturn err;\n",
    "}\n",
    "```\n",
    "\n",
    "1. `next = (address + PGDIR_SIZE) & PGDIR_MASK;` 第一级是`PGDIR_SIZE = (1<<26)`的\n",
    "\n",
    "2. 下面这些函数逐级在page table上建立item\n",
    "\n",
    "```c\n",
    "static int map_area_pte(pte_t *pte, unsigned long address,\n",
    "\t\t\t       unsigned long size, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tunsigned long end;\n",
    "\n",
    "\taddress &= ~PMD_MASK;\n",
    "\tend = address + size;\n",
    "\tif (end > PMD_SIZE)\n",
    "\t\tend = PMD_SIZE;\n",
    "\n",
    "\tdo {\n",
    "\t\tstruct page *page = **pages;\n",
    "\t\tWARN_ON(!pte_none(*pte));\n",
    "\t\tif (!page)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\n",
    "\t\tset_pte(pte, mk_pte(page, prot));\n",
    "\t\taddress += PAGE_SIZE;\n",
    "\t\tpte++;\n",
    "\t\t(*pages)++;\n",
    "\t} while (address < end);\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "static int map_area_pmd(pmd_t *pmd, unsigned long address,\n",
    "\t\t\t       unsigned long size, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tunsigned long base, end;\n",
    "\n",
    "\tbase = address & PUD_MASK;\n",
    "\taddress &= ~PUD_MASK;\n",
    "\tend = address + size;\n",
    "\tif (end > PUD_SIZE)\n",
    "\t\tend = PUD_SIZE;\n",
    "\n",
    "\tdo {\n",
    "\t\tpte_t * pte = pte_alloc_kernel(&init_mm, pmd, base + address);\n",
    "\t\tif (!pte)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tif (map_area_pte(pte, address, end - address, prot, pages))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\taddress = (address + PMD_SIZE) & PMD_MASK;\n",
    "\t\tpmd++;\n",
    "\t} while (address < end);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "static int map_area_pud(pud_t *pud, unsigned long address,\n",
    "\t\t\t       unsigned long end, pgprot_t prot,\n",
    "\t\t\t       struct page ***pages)\n",
    "{\n",
    "\tdo {\n",
    "\t\tpmd_t *pmd = pmd_alloc(&init_mm, pud, address);\n",
    "\t\tif (!pmd)\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\tif (map_area_pmd(pmd, address, end - address, prot, pages))\n",
    "\t\t\treturn -ENOMEM;\n",
    "\t\taddress = (address + PUD_SIZE) & PUD_MASK;\n",
    "\t\tpud++;\n",
    "\t} while (address && address < end);\n",
    "\n",
    "\treturn 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paging directory\n",
    "\n",
    "![paging01](resources/paging01.png)\n",
    "![paging02](resources/paging02.png)\n",
    "![paging03](resources/paging03.png)\n",
    "![paging03](resources/pagedesp03.png)\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Frame Management\n",
    "\n",
    "### page descriptors\n",
    "\n",
    "![pagedesp01](resources/pagedesp01.png)\n",
    "![pagedesp02](resources/pagedesp02.png)\n",
    "\n",
    "1. `mem_map` 是个 `page` 的array。在numa架构下，\n",
    "\n",
    "### linux2.6/mm/memory.c\n",
    "\n",
    "```c\n",
    "#ifndef CONFIG_DISCONTIGMEM\n",
    "/* use the per-pgdat data instead for discontigmem - mbligh */\n",
    "unsigned long max_mapnr;\n",
    "struct page *mem_map;\n",
    "\n",
    "EXPORT_SYMBOL(max_mapnr);\n",
    "EXPORT_SYMBOL(mem_map);\n",
    "#endif\n",
    "\n",
    "```\n",
    "\n",
    "### linux2.6/include/linux/mm.h\n",
    "\n",
    "```c\n",
    "/*\n",
    " * Each physical page in the system has a struct page associated with\n",
    " * it to keep track of whatever it is we are using the page for at the\n",
    " * moment. Note that we have no way to track which tasks are using\n",
    " * a page.\n",
    " */\n",
    "struct page {\n",
    "\tpage_flags_t flags;\t\t/* Atomic flags, some possibly\n",
    "\t\t\t\t\t * updated asynchronously */\n",
    "\tatomic_t _count;\t\t/* Usage count, see below. */\n",
    "\tatomic_t _mapcount;\t\t/* Count of ptes mapped in mms,\n",
    "\t\t\t\t\t * to show when page is mapped\n",
    "\t\t\t\t\t * & limit reverse map searches.\n",
    "\t\t\t\t\t */\n",
    "\tunsigned long private;\t\t/* Mapping-private opaque data:\n",
    "\t\t\t\t\t * usually used for buffer_heads\n",
    "\t\t\t\t\t * if PagePrivate set; used for\n",
    "\t\t\t\t\t * swp_entry_t if PageSwapCache\n",
    "\t\t\t\t\t * When page is free, this indicates\n",
    "\t\t\t\t\t * order in the buddy system.\n",
    "\t\t\t\t\t */\n",
    "\tstruct address_space *mapping;\t/* If low bit clear, points to\n",
    "\t\t\t\t\t * inode address_space, or NULL.\n",
    "\t\t\t\t\t * If page mapped as anonymous\n",
    "\t\t\t\t\t * memory, low bit is set, and\n",
    "\t\t\t\t\t * it points to anon_vma object:\n",
    "\t\t\t\t\t * see PAGE_MAPPING_ANON below.\n",
    "\t\t\t\t\t */\n",
    "\tpgoff_t index;\t\t\t/* Our offset within mapping. */\n",
    "\tstruct list_head lru;\t\t/* Pageout list, eg. active_list\n",
    "\t\t\t\t\t * protected by zone->lru_lock !\n",
    "\t\t\t\t\t */\n",
    "\t/*\n",
    "\t * On machines where all RAM is mapped into kernel address space,\n",
    "\t * we can simply calculate the virtual address. On machines with\n",
    "\t * highmem some memory is mapped into kernel virtual memory\n",
    "\t * dynamically, so we need a place to store that address.\n",
    "\t * Note that this field could be 16 bits on x86 ... ;)\n",
    "\t *\n",
    "\t * Architectures with slow multiplication can define\n",
    "\t * WANT_PAGE_VIRTUAL in asm/page.h\n",
    "\t */\n",
    "#if defined(WANT_PAGE_VIRTUAL)\n",
    "\tvoid *virtual;\t\t\t/* Kernel virtual address (NULL if\n",
    "\t\t\t\t\t   not kmapped, ie. highmem) */\n",
    "#endif /* WANT_PAGE_VIRTUAL */\n",
    "};\n",
    "\n",
    "```\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUMA\n",
    "\n",
    "![numa01](resources/numa01.png)\n",
    "![numa02](resources/numa02.png)\n",
    "\n",
    "-----------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d47d1382e92bbaf84c50276baa079056532326c20bdaac4b09430c41eda0c22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
